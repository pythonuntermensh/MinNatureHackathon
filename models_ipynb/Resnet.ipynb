{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b62f7769-ebd5-42e7-81f9-55f738ad635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "#from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input, decode_predictions\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4cd46-4bb9-48f7-90d1-c901eb8a2ab8",
   "metadata": {},
   "source": [
    "# DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293b83d-f6f3-4138-91b5-4efe3a93f2a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fa270f5-6bf6-4249-84ec-11ac75a6b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = 'data/train/'\n",
    "VALIDATION_DATA_DIR = 'data/valid/'\n",
    "TRAIN_SAMPLES = 2000\n",
    "VALIDATION_SAMPLES = 360\n",
    "NUM_CLASSES = 3\n",
    "IMG_WIDTH, IMG_HEIGHT = 224, 224\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e91fb23f-ac02-428d-b030-663633d103b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,\n",
    "                                   rotation_range=20,\n",
    "                                   width_shift_range=0.2,\n",
    "                                   height_shift_range=0.2,\n",
    "                                   zoom_range=0.2)\n",
    "val_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b282699-3de4-4811-9db5-9f9a4214beb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10604 images belonging to 3 classes.\n",
      "Found 1848 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                        TRAIN_DATA_DIR,\n",
    "                        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=True,\n",
    "                        seed=12345,\n",
    "                        class_mode='categorical')\n",
    "validation_generator = val_datagen.flow_from_directory(\n",
    "                        VALIDATION_DATA_DIR,\n",
    "                        target_size=(IMG_WIDTH, IMG_HEIGHT),\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        shuffle=False,\n",
    "                        class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ef8675fe-17e9-45db-993c-d74fbbd41bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_maker():\n",
    "    base_model = ResNet50(include_top=False, input_shape =\n",
    "                    (IMG_WIDTH,IMG_HEIGHT,3))\n",
    "    for layer in base_model.layers[:]:\n",
    "        layer.trainable = False # Freeze the layers\n",
    "    input = Input(shape=(IMG_WIDTH, IMG_HEIGHT, 3))\n",
    "    custom_model = base_model(input)\n",
    "    custom_model = GlobalAveragePooling2D()(custom_model)\n",
    "    custom_model = Dense(64, activation='relu')(custom_model)\n",
    "    custom_model = Dropout(0.5)(custom_model)\n",
    "    predictions = Dense(NUM_CLASSES, activation='softmax')(custom_model)\n",
    "    return Model(inputs=input, outputs=predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b0aa8a8-ea01-466d-91ec-7ef9ceea0c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pendo\\AppData\\Local\\Temp\\ipykernel_23536\\3648776649.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "32/32 [==============================] - ETA: 0s - loss: 0.8200 - acc: 0.6704WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 32 batches). You may need to use the repeat() function when building your dataset.\n",
      "32/32 [==============================] - 40s 1s/step - loss: 0.8200 - acc: 0.6704 - val_loss: 0.8423 - val_acc: 0.6483\n",
      "Epoch 2/30\n",
      "32/32 [==============================] - 26s 802ms/step - loss: 0.6443 - acc: 0.7559\n",
      "Epoch 3/30\n",
      "32/32 [==============================] - 24s 747ms/step - loss: 0.5975 - acc: 0.7729\n",
      "Epoch 4/30\n",
      "32/32 [==============================] - 26s 801ms/step - loss: 0.5588 - acc: 0.7915\n",
      "Epoch 5/30\n",
      "32/32 [==============================] - 24s 730ms/step - loss: 0.5355 - acc: 0.8077\n",
      "Epoch 6/30\n",
      "32/32 [==============================] - 22s 664ms/step - loss: 0.4950 - acc: 0.8232\n",
      "Epoch 7/30\n",
      "32/32 [==============================] - 21s 658ms/step - loss: 0.5298 - acc: 0.8125\n",
      "Epoch 8/30\n",
      "32/32 [==============================] - 21s 634ms/step - loss: 0.5073 - acc: 0.8164\n",
      "Epoch 9/30\n",
      "32/32 [==============================] - 21s 643ms/step - loss: 0.5301 - acc: 0.8166\n",
      "Epoch 10/30\n",
      "32/32 [==============================] - 21s 646ms/step - loss: 0.5044 - acc: 0.8188\n",
      "Epoch 11/30\n",
      "32/32 [==============================] - 23s 699ms/step - loss: 0.5272 - acc: 0.8130\n",
      "Epoch 12/30\n",
      "32/32 [==============================] - 22s 682ms/step - loss: 0.5117 - acc: 0.8149\n",
      "Epoch 13/30\n",
      "32/32 [==============================] - 22s 672ms/step - loss: 0.5096 - acc: 0.8247\n",
      "Epoch 14/30\n",
      "32/32 [==============================] - 21s 641ms/step - loss: 0.4780 - acc: 0.8296\n",
      "Epoch 15/30\n",
      "32/32 [==============================] - 21s 633ms/step - loss: 0.4955 - acc: 0.8154\n",
      "Epoch 16/30\n",
      "32/32 [==============================] - 21s 657ms/step - loss: 0.4676 - acc: 0.8379\n",
      "Epoch 17/30\n",
      "32/32 [==============================] - 21s 658ms/step - loss: 0.4660 - acc: 0.8340\n",
      "Epoch 18/30\n",
      "32/32 [==============================] - 21s 644ms/step - loss: 0.4927 - acc: 0.8267\n",
      "Epoch 19/30\n",
      "32/32 [==============================] - 21s 638ms/step - loss: 0.4509 - acc: 0.8418\n",
      "Epoch 20/30\n",
      "32/32 [==============================] - 21s 643ms/step - loss: 0.4628 - acc: 0.8427\n",
      "Epoch 21/30\n",
      "32/32 [==============================] - 21s 652ms/step - loss: 0.4750 - acc: 0.8208\n",
      "Epoch 22/30\n",
      "32/32 [==============================] - 23s 718ms/step - loss: 0.4638 - acc: 0.8271\n",
      "Epoch 23/30\n",
      "32/32 [==============================] - 21s 648ms/step - loss: 0.4839 - acc: 0.8296\n",
      "Epoch 24/30\n",
      "32/32 [==============================] - 21s 652ms/step - loss: 0.4698 - acc: 0.8218\n",
      "Epoch 25/30\n",
      "32/32 [==============================] - 22s 665ms/step - loss: 0.4548 - acc: 0.8364\n",
      "Epoch 26/30\n",
      "32/32 [==============================] - 21s 646ms/step - loss: 0.4449 - acc: 0.8447\n",
      "Epoch 27/30\n",
      "32/32 [==============================] - 21s 653ms/step - loss: 0.4844 - acc: 0.8374\n",
      "Epoch 28/30\n",
      "32/32 [==============================] - 21s 655ms/step - loss: 0.4360 - acc: 0.8481\n",
      "Epoch 29/30\n",
      "32/32 [==============================] - 21s 651ms/step - loss: 0.4457 - acc: 0.8369\n",
      "Epoch 30/30\n",
      "32/32 [==============================] - 21s 661ms/step - loss: 0.4411 - acc: 0.8501\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2498b436b60>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model_maker()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer= tensorflow.keras.optimizers.Adam(lr=0.001),\n",
    "              metrics=['acc'])\n",
    "num_steps = math.ceil(float(TRAIN_SAMPLES)/BATCH_SIZE)\n",
    "model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = num_steps,\n",
    "                    epochs=30,\n",
    "                    validation_data = validation_generator,\n",
    "                    validation_steps = num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b125261e-466a-4c0a-b1d3-aa3ac1e7e01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "[[0.30625647 0.28725535 0.4064882 ]]\n",
      "{'0': 0, '1': 1, '2': 2}\n"
     ]
    }
   ],
   "source": [
    "img_path = 'sample_images/Олень6.jpg'\n",
    "img = tensorflow.keras.preprocessing.image.load_img(img_path, target_size=(224,224))\n",
    "img_array = tensorflow.keras.preprocessing.image.img_to_array(img)\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0)\n",
    "preprocessed_img = preprocess_input(expanded_img_array) # Preprocess the image\n",
    "prediction = model.predict(preprocessed_img)\n",
    "print(prediction)\n",
    "print(validation_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc3a1603-a97f-47db-baf8-b722ef0ee933",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a2549f-6e74-4153-a966-3983cbcddbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
